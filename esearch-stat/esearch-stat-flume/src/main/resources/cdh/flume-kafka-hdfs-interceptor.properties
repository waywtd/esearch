# 自定义sources的名字
flumetohdfs_agent.sources = source_from_kafka
# 自定义channels的名字
flumetohdfs_agent.channels = mem_channel
# 自定义sinks的名字
flumetohdfs_agent.sinks = hdfs_sink
#auto.commit.enable = true

# 指定source使用的channel名字
flumetohdfs_agent.sources.source_from_kafka.channels = mem_channel
# 指定sink需要使用的channel的名字,注意这里是channel
flumetohdfs_agent.sinks.hdfs_sink.channel = mem_channel

# -------- hdfsKafkaSource相关配置-----------------
# 定义消息源类型
flumetohdfs_agent.sources.source_from_kafka.type = org.apache.flume.source.kafka.KafkaSource
# 定义kafka所在zk的地址
flumetohdfs_agent.sources.source_from_kafka.zookeeperConnect = 10.171.107.29:2181,10.171.107.11:2181,10.171.107.26:2181/kafka2
# kafka 的brokers
flumetohdfs_agent.sources.source_from_kafka.kafka.bootstrap.servers = 10.177.239.39:9092,10.177.239.54:9092,10.177.239.196:9092
# 配置消费的kafka topic  多个逗号分隔
flumetohdfs_agent.sources.source_from_kafka.kafka.topics = cdn-sl-logsss-com
# 配置消费的kafka topic 正则
#flumetohdfs_agent.sources.source_from_kafka.kafka.topics.regex = ^cdn-sl.*
# 序列化
flumetohdfs_agent.sources.source_from_kafka.useFlumeEventFormat = false


# 配置消费者组的id
flumetohdfs_agent.sources.source_from_kafka.kafka.consumer.group.id = esearch
# 达到多少条传输
flumetohdfs_agent.sources.source_from_kafka.batchSize = 1000
# 达到多长时间传输
flumetohdfs_agent.sources.source_from_kafka.batchDurationMillis = 2000

# 自定义拦截器
flumetohdfs_agent.sources.source_from_kafka.interceptors = i1
flumetohdfs_agent.sources.source_from_kafka.interceptors.i1.type = org.globalegrow.esearch.interceptor.LogInterceptor$Builder

# ---------Sink 相关配置------------------
flumetohdfs_agent.sinks.hdfs_sink.type = hdfs
flumetohdfs_agent.sinks.hdfs_sink.hdfs.path = hdfs:///globalegrow/%{log_type}-log/%{year}/%{month}/%{day}/%{domain}
## roll every hour (after gz)
flumetohdfs_agent.sinks.hdfs_sink.hdfs.rollInterval = 0
flumetohdfs_agent.sinks.hdfs_sink.hdfs.threadsPoolSize = 30
# 配置前缀和后缀
flumetohdfs_agent.sinks.hdfs_sink.hdfs.filePrefix = %{log_type}.%{year}-%{month}-%{day}
flumetohdfs_agent.sinks.hdfs_sink.hdfs.fileSuffix = .log
flumetohdfs_agent.sinks.hdfs_sink.hdfs.fileType = DataStream
flumetohdfs_agent.sinks.hdfs_sink.hdfs.writeFormat = Text
flumetohdfs_agent.sinks.hdfs_sink.hdfs.minBlockReplicas = 1
flumetohdfs_agent.sinks.hdfs_sink.hdfs.rollSize = 132692539
flumetohdfs_agent.sinks.hdfs_sink.hdfs.idleTimeout = 600
flumetohdfs_agent.sinks.hdfs_sink.hdfs.batchSize = 500
# 当临时文件达到多少条数生成新的文件,设置0标识不根据数量来分割文件
flumetohdfs_agent.sinks.hdfs_sink.hdfs.rollCount = 0
flumetohdfs_agent.sinks.hdfs_sink.hdfs.round = true
flumetohdfs_agent.sinks.hdfs_sink.hdfs.roundValue = 10

flumetohdfs_agent.sinks.hdfs_sink.hdfs.roundUnit = minute
flumetohdfs_agent.sinks.hdfs_sink.hdfs.rollTimerPoolSize = 3
#使用本地时间
flumetohdfs_agent.sinks.hdfs_sink.hdfs.useLocalTimeStamp = true

# ------- fileChannel相关配置-------------------------
flumetohdfs_agent.channels.mem_channel.type = SPILLABLEMEMORY
# channel存储的事件容量
#flumetohdfs_agent.channels.memoryCapacity = 500000
#flumetohdfs_agent.channels.overflowCapacity = 1000000
#flumetohdfs_agent.channels.byteCapacity = 800000
# 文件目录
#flumetohdfs_agent.channels.checkpointDir =  /data/flume/checkpoint
#flumetohdfs_agent.channels.dataDirs =  /data/flume/data


